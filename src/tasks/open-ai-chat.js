/**
 * @file A collection of functions for executing chat completions with the OpenAI API
 * @author Thomas J. Purk
 */

import OpenAI from "openai";
import fs from "fs";
import { zodResponseFormat } from "openai/helpers/zod";
import { responseSchema } from "./open-ai-chat-schema.js";
const openai = new OpenAI();

export async function completeChat(
  systemContent,
  userContent,
  parentConcept,
  concept,
  dirCompletions
) {
  // Determine if a file containing a previous chat completion already exists
  // If it does, load the data from the file rather than submitting a new
  // chat completion to the LLM.
  let inputFile = `${dirCompletions}/${parentConcept}/${concept}.json`;
  if (fs.existsSync(inputFile)) {
    console.log(`Loading From File: ${concept}`);
    return fs.readFileSync(inputFile).toString();
  } else {
    userContent = userContent.replace("<name>", concept);
    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        { role: "system", content: systemContent },
        {
          role: "user",
          content: userContent,
        },
      ],
      response_format: zodResponseFormat(responseSchema[parentConcept], parentConcept),
    });
    console.log(`Generated By LLM: ${concept}`);
    return completion.choices[0].message.content;
  }
}
