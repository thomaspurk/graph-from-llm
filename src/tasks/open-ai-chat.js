/**
 * @file A collection of functions for executing chat completions with the OpenAI API
 * @author Thomas J. Purk
 */

import OpenAI from "openai";
import fs from "fs";
import { zodResponseFormat } from "openai/helpers/zod";
import { responseSchema } from "./open-ai-chat-schema.js";
const openai = new OpenAI();

export async function completeChat(
  systemContent,
  userContent,
  parentConcept,
  conceptName,
  dirCompletions
) {
  // Determine if a file containing a previous chat completion already exists
  // If it does, load the data from the file rather than submitting a new
  // chat completion to the LLM.
  let targetFile = `${dirCompletions}/${parentConcept}/${conceptName}.json`;
  if (fs.existsSync(targetFile)) {
    console.log(`Loading From File: ${conceptName}`);
    return fs.readFileSync(targetFile).toString();
  } else {
    userContent = userContent.replace("<name>", conceptName);
    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        { role: "system", content: systemContent },
        {
          role: "user",
          content: userContent,
        },
      ],
      response_format: zodResponseFormat(responseSchema[parentConcept], parentConcept),
    });
    const conceptJsonString = completion.choices[0].message.content;
    console.log(`Generated By LLM: ${conceptName}`);

    // Write the LLM data to a cache file for future use
    fs.writeFileSync(targetFile, conceptJsonString);

    // Return the JSON string to the calling function.
    return conceptJsonString;
  }
}
